{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installing libraries"
      ],
      "metadata": {
        "id": "7ZJ7tRmcavPa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1uT-5sX5Pu6"
      },
      "outputs": [],
      "source": [
        "%pip install -qU langchain_openai langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI API key Setup"
      ],
      "metadata": {
        "id": "I98sVr9J5fUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "4Mmt5wP_5kwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM Configuration"
      ],
      "metadata": {
        "id": "ttoRMJn2a2th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.globals import set_llm_cache\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")"
      ],
      "metadata": {
        "id": "fc9FiEJY5xnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Response generation without Caching"
      ],
      "metadata": {
        "id": "qHSJXytKa5JA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from langchain.cache import InMemoryCache\n",
        "\n",
        "set_llm_cache(InMemoryCache())\n",
        "\n",
        "# The first time, it is not yet in cache, so it should take longer\n",
        "llm.invoke(\"What is memory caching? Explain in less than 100 words\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "HC21bp_559VZ",
        "outputId": "5e8ce019-e179-4366-e811-9d3caaa013fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 35 ms, sys: 1.13 ms, total: 36.2 ms\n",
            "Wall time: 1.8 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nMemory caching is a technique used in computer systems to improve the performance of accessing data. It involves storing frequently used data in a faster and closer location, such as the computer's main memory, rather than retrieving it from a slower and more distant location, such as the hard drive. This allows for quicker access to data, reducing the time and resources needed to retrieve it. When data is requested, the system checks the cache first and if the data is found, it is retrieved from the cache instead of the original location. This results in faster data retrieval and improved overall system performance.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Response generation with Caching"
      ],
      "metadata": {
        "id": "Sfy0Pehpa8Fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# The second time it is, so it goes faster\n",
        "llm.invoke(\"What is memory caching? Explain in less than 100 words\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "gdWIi-2S6LcG",
        "outputId": "43186fbc-866f-4d5e-e4d5-26b96830e985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 716 µs, sys: 0 ns, total: 716 µs\n",
            "Wall time: 752 µs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nMemory caching is a technique used in computer systems to improve the performance of accessing data. It involves storing frequently used data in a faster and closer location, such as the computer's main memory, rather than retrieving it from a slower and more distant location, such as the hard drive. This allows for quicker access to data, reducing the time and resources needed to retrieve it. When data is requested, the system checks the cache first and if the data is found, it is retrieved from the cache instead of the original location. This results in faster data retrieval and improved overall system performance.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Caching Embeddings"
      ],
      "metadata": {
        "id": "Kcq_bp9oFVPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain-openai faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxHBQOWe7QvX",
        "outputId": "e0b59b70-9a7d-4113-e558-fc7162d272c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.storage import LocalFileStore\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain.embeddings.cache import CacheBackedEmbeddings\n",
        "\n",
        "underlying_embeddings = OpenAIEmbeddings()\n",
        "\n",
        "store = LocalFileStore(\"./cache/\")\n",
        "\n",
        "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    underlying_embeddings, store, namespace=underlying_embeddings.model\n",
        ")\n",
        "\n",
        "list(store.yield_keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k_K1MZOFW-X",
        "outputId": "d4dd7e3e-c0a5-4427-ea40-26b4f06ef326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_documents = TextLoader(\"GOT_script.txt\").load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "documents = text_splitter.split_documents(raw_documents)"
      ],
      "metadata": {
        "id": "AULmPotnFgKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### creating the vector embeddings"
      ],
      "metadata": {
        "id": "nRtpeuU1GIO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "db = FAISS.from_documents(documents, cached_embedder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPd1UaV2F17z",
        "outputId": "6c292dae-f4c4-4079-f65c-60836721d793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 819 ms, sys: 46.7 ms, total: 866 ms\n",
            "Wall time: 1.71 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### creating the vector store again will be faster as we have employed embedding caching and there is no need for recomputation of vectors"
      ],
      "metadata": {
        "id": "YF2iHLudF9py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "db2 = FAISS.from_documents(documents, cached_embedder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_CI8UIZF4Z8",
        "outputId": "266df70d-7779-4bd2-c9a0-ae994e9d35e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4.74 ms, sys: 56 µs, total: 4.79 ms\n",
            "Wall time: 12 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(store.yield_keys())[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jaa8U3amF7C3",
        "outputId": "d7baa517-c9d2-4b83-ae59-5c831293a223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['text-embedding-ada-002cb0f15e2-ae17-50ef-809e-cdd44eb36665',\n",
              " 'text-embedding-ada-0023fea82f3-f0a6-5b24-84cb-cefe7dd8ba8b']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8TV2-WlCF9DN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}