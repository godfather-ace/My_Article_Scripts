{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Library Installation"
      ],
      "metadata": {
        "id": "iNkjucsn3euQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U --quiet llama-index llama-index-llms-openai llama-index-readers-web pyvis"
      ],
      "metadata": {
        "id": "BP-6x7f9yIQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting the OpenAI API Key"
      ],
      "metadata": {
        "id": "mNSqdJXg3c8c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLKESU8Ax2s9"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Event"
      ],
      "metadata": {
        "id": "hRWMV3cP3QBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import Event\n",
        "from llama_index.core.schema import NodeWithScore\n",
        "\n",
        "class RetrieverEvent(Event):\n",
        "    \"\"\"Result of running retrieval\"\"\"\n",
        "\n",
        "    nodes: list[NodeWithScore]"
      ],
      "metadata": {
        "id": "oyzRdIuoy54p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Workflow\n",
        "\n"
      ],
      "metadata": {
        "id": "31xV_Zwf3MJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "from llama_index.core.response_synthesizers import CompactAndRefine\n",
        "from llama_index.core.workflow import (\n",
        "    Context,\n",
        "    Workflow,\n",
        "    StartEvent,\n",
        "    StopEvent,\n",
        "    step,\n",
        ")\n",
        "\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "\n",
        "class RAGWorkflow(Workflow):\n",
        "    @step(pass_context=True)\n",
        "    async def ingest(self, ctx: Context, ev: StartEvent) -> StopEvent | None:\n",
        "        \"\"\"Entry point to ingest a document, triggered by a StartEvent with `dirname`.\"\"\"\n",
        "        dirname = ev.get(\"dirname\")\n",
        "        if not dirname:\n",
        "            return None\n",
        "\n",
        "        documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
        "            [\"https://jhmhp.amegroups.org/article/view/8842/html\"])\n",
        "        ctx.data[\"index\"] = VectorStoreIndex.from_documents(\n",
        "            documents=documents,\n",
        "            embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
        "        )\n",
        "        return StopEvent(result=f\"Indexed {len(documents)} documents.\")\n",
        "\n",
        "    @step(pass_context=True)\n",
        "    async def retrieve(\n",
        "        self, ctx: Context, ev: StartEvent\n",
        "    ) -> RetrieverEvent | None:\n",
        "        \"RAG Entry, triggered by a StartEvent with `query`.\"\n",
        "        query = ev.get(\"query\")\n",
        "        if not query:\n",
        "            return None\n",
        "\n",
        "        print(f\"Query the database with: {query}\")\n",
        "\n",
        "        # store the query in the global context\n",
        "        ctx.data[\"query\"] = query\n",
        "\n",
        "        # get the index from the global context\n",
        "        index = ctx.data.get(\"index\")\n",
        "        if index is None:\n",
        "            print(\"Index is empty, load some documents before querying!\")\n",
        "            return None\n",
        "\n",
        "        retriever = index.as_retriever(similarity_top_k=2)\n",
        "        nodes = retriever.retrieve(query)\n",
        "        print(f\"Retrieved {len(nodes)} nodes.\")\n",
        "        return RetrieverEvent(nodes=nodes)\n",
        "\n",
        "    @step(pass_context=True)\n",
        "    async def synthesize(self, ctx: Context, ev: RetrieverEvent) -> StopEvent:\n",
        "        llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "        summarizer = CompactAndRefine(llm=llm, streaming=True, verbose=True)\n",
        "        query = ctx.data.get(\"query\")\n",
        "\n",
        "        response = await summarizer.asynthesize(query, nodes=ev.nodes)\n",
        "        return StopEvent(result=response)"
      ],
      "metadata": {
        "id": "7Di7dkAJz6os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Executing the workflow and drawing the flow graph"
      ],
      "metadata": {
        "id": "57aFcUYr3Hvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import (\n",
        "    draw_all_possible_flows,\n",
        "    draw_most_recent_execution,\n",
        ")\n",
        "\n",
        "draw_all_possible_flows(RAGWorkflow, filename=\"RAG_flow.html\")\n",
        "\n",
        "w = RAGWorkflow()\n",
        "await w.run(dirname=\"data\")\n",
        "result = await w.run(query=\"What is the data about?\")\n",
        "async for chunk in result.async_response_gen():\n",
        "    print(chunk, end=\"\", flush=True)\n",
        "draw_most_recent_execution(w, filename=\"rag_flow_recent.html\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lio-cmdm0RDT",
        "outputId": "57e9c6b6-88fa-46e8-bfa2-62d4bd5dceb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG_flow.html\n",
            "Query the database with: What is the data about?\n",
            "Retrieved 2 nodes.\n",
            "The data pertains to a narrative review focused on advancing the democratization of generative artificial intelligence in healthcare. It includes references to various studies and articles related to machine learning and risk prediction in healthcare settings. The review is published in the Journal of Hospital Management and Health Policy.rag_flow_recent.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f1oPyIlk5elY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wzfqplnR8MOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l_f0dNMx9-De"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}